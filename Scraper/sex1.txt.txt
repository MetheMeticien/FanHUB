import requests
from bs4 import BeautifulSoup
import csv

def scrape_news_paragraphs(url):
    try:
        page = requests.get(url)
        page.raise_for_status() 
    except requests.exceptions.RequestException as e:
        print(f"Error fetching the URL: {e}")
        return None
    
    
    soup = BeautifulSoup(page.text, 'lxml')
    
    try:
        title = soup.title.string.strip()
    except AttributeError:
        title = "No title found"
    
    paragraphs = soup.find_all('p')
    article_body = "\n\n".join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
    
    if not article_body:
        article_body = "No content found in <p> tags."
    
    return article_body


url = 'https://www.skysports.com/'


page = requests.get(url)
soup = BeautifulSoup(page.text, 'html.parser')


news_headlines = soup.find_all('h3')


news_data = []

for headline in news_headlines:
    a_tag = headline.find('a')  
    if a_tag:
        news_title = a_tag.get_text().strip()
        news_link = a_tag['href']

        if not news_link.startswith('http'):
            news_link = f'https://www.skysports.com{news_link}'

        print(f"Scraping news: {news_title}")
        
        article_content = scrape_news_paragraphs(news_link)
        
        if article_content:
            news_data.append((news_title, article_content))

csv_file = 'news_headlines.csv'
with open(csv_file, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    # Write the header row
    writer.writerow(['News Title', 'Paragraphs'])
    
    # Write the news data
    for title, content in news_data:
        writer.writerow([title, content])

print(f"Data has been written to {csv_file}")





import requests
from bs4 import BeautifulSoup
import csv

def extract_articles(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "Referer": "https://www.google.com",
    }

    articles = []

    # Send the request to the website
    response = requests.get(url, headers=headers)

    # Check if the request was successful
    if response.status_code == 403:
        print("Access forbidden.")
        return articles
    elif response.status_code == 200:
        soup = BeautifulSoup(response.content, "html.parser")

        # Find all article links
        article_links = soup.find_all('a', href=True)

        for link in article_links:
            href = link['href']
            if href.startswith('/'):
                href = url + href  # Convert relative path to full URL
            
            # Filter for relevant article links
            if 'article' in href or 'story' in href:
                articles.append(href)

    else:
        print(f"Failed to retrieve the page. Status code: {response.status_code}")
    
    # print(articles)
    return articles

def extract_story(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract the header/title
        header = soup.find('h1')
        if header:
            header = header.get_text(strip=True)
        else:
            header = "No Header Found"

        # Extract the story content
        story_content = soup.find_all('p')
        story = '\n'.join([paragraph.get_text(strip=True) for paragraph in story_content if paragraph.get_text(strip=True)])

        if not story:
            story = "No Story Content Found"
        
        return header, story
    else:
        print(f"Failed to retrieve the story. Status code: {response.status_code}")
        return None, None

csv_file = "ESPN.csv"
def save_to_csv(articles):
    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
    # Write the header row
        writer.writerow(['News Title', 'Paragraphs'])
        
        # Write the news data
        for article in articles:
            header, story = extract_story(article)
            if header and story:
                writer.writerow([header, story])
    # Write each article's header and story to the CSV

# Example usage
url = "https://www.espn.in"
articles = extract_articles(url)

print(extract_story("https://www.espn.in/olympics/story/_/id/40609447/india-olympics-paris-2024-latest-news-schedule-features-videos-analysis"));

if articles:
    # save_to_csv(articles)
    print("Data saved to ESPN.csv.")
else:
    print("No articles found.")